{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80be8116",
   "metadata": {},
   "source": [
    "# Activation fuctions in nueral networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d74b5",
   "metadata": {},
   "source": [
    "<h2> 1 -> Sigmoid or Logistic Activation Function </h2>\n",
    "<p> Sigmoid chart look like this <img src=\"sigmoid.png\" width=400 height=400 /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ed806",
   "metadata": {},
   "source": [
    "So the sigmoid function is best used when we have to pedict the<b> probability of the output</b>.Science the existance o probaility between 0 to 1 sigmoid is the right choice\n",
    "\n",
    "<p>This is used for the last layer in a nueral network</p>\n",
    "\n",
    "<p>The logistic sigmoid function can cause a neural network to get stuck at the training time.\n",
    "The softmax function is a more generalized logistic activation function which is used for multiclass classification.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd918807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc1c27d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.7310585786300049, 3.305700626760734e-37, 0.5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(100), sigmoid(1), sigmoid(-84), sigmoid(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037993e5",
   "metadata": {},
   "source": [
    "<h2> 2 -> Tanh or Hyperbolic tangent activation function </h2>\n",
    "<p>Tanh activation function ranges from -1 to +1 rather sigmoid activation function is ranges from 0 to 1  <img src=\"tanh.jpeg\" width=400 height=400 /></p>\n",
    "<p> (e^x) - (e^-x) / (e^x) + (e^-x)   </p>\n",
    "\n",
    "<p>And tanH activation functions are used in feed forward nets  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a18dbecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "  return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a34a58e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.7615941559557649, -1.0, 0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(100), tanh(1), tanh(-84), tanh(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133f534",
   "metadata": {},
   "source": [
    "<h1> 3-> rectified linear activation function or ReLU</h1>\n",
    "<p>RelU is the most widely used activation functions and it is best suited for hidden layers<img src=\"relu.jpeg\" width=400 height=400 /></p>\n",
    "<p> ma(0, z)  </p>\n",
    "\n",
    "<p>But there is an issue in the relun activation function like the negative values beacame zero immediatly.Which lleads to dificulty to reach global minima point, also called dying relu </p>\n",
    "<p> ReLU learns much faster than sigmoid and Tanh function.</P>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d242600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "645cbb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ca85e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57f303",
   "metadata": {},
   "source": [
    "<h1> 4 -> Leaky ReLU Function </h1>\n",
    "<p> LeakyReLU is an improved version of relu activation function.And it is solved the dying relu proble as it has a small positive slope in the negative area.</p>\n",
    "<p><img src=\"leaky-relu.jpg\" width=400 height=400 /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df825336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return max(0.1*x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e972d6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10.0, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu(-100), leaky_relu(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049d85a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
